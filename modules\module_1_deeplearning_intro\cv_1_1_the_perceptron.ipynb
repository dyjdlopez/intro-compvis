{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbrLLIPt9FCoI3oTQKa+Cs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyjdlopez/intro-compvis/blob/main/modules%5Cmodule_1_deeplearning_intro%5Ccv_1_1_the_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNe45Gc3B089"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1.1 Machine Learning Review\n",
        "Copyright (c) D.Lopez 2024 | All Rights reserved <br><br>\n",
        "The main difference between machine learning and traditional programming is how a solution is created.<br>\n",
        "<b>Traditional Programming</b><br>\n",
        "In traditional programming or the usual way, when we code a solution, we start with a set of given inputs and some rules we code so that when we run our program with the set of inputs, we get some desirable output.<br>\n",
        "<b>Machine Learning</b><br>\n",
        "In machine learning programming, we have a set of inputs and outputs, and we try to determine a rule, pattern, or equation that will describe their relationship.\n",
        "\n",
        "![image](https://raw.githubusercontent.com/JiaRuiShao/TensorFlow/master/1-Introduction%20to%20Tensorflow%20for%20AI%2C%20ML%20and%20DL/images/W1.1.PNG?raw=true)<br>\n",
        "\n",
        "In this module, we will have a start with machine learning. Weâ€™ll learn about datasets and the learning algorithms that we can use to recognize patterns between datasets."
      ],
      "metadata": {
        "id": "DnAnR2TIB9lR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Datasets\n",
        "Datasets consist of data that are relevant to a certain scenario or subject of interest. It may contain numerical, text, image data, or a mix of them. Datasets, depending on where they are obtained, need to be cleaned and transformed to fit your needs."
      ],
      "metadata": {
        "id": "LryNPADaCaXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Pandas\n",
        "\n",
        "Another tool to add to the machine learning engineer's toolbox is Pandas. [Pandas](https://pandas.pydata.org/docs/#module-pandas) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. <br>\n",
        "Check out:\n",
        "* [Setting up DataFrames in Pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html#min-tut-01-tableoriented)\n",
        "* [Reading and writing Data](https://pandas.pydata.org/docs/getting_started/intro_tutorials/02_read_write.html#min-tut-02-read-write)\n",
        "* [Summarizing Stastics of a Dataset](https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html#min-tut-06-stats)"
      ],
      "metadata": {
        "id": "oQEYVEmPCcgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "rMgjSsIIB-iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use a preset dataset available in Google Colab\n",
        "ds = pd.read_csv('/content/sample_data/california_housing_train.csv')"
      ],
      "metadata": {
        "id": "ndFlz3JCCy6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.keys()"
      ],
      "metadata": {
        "id": "MjU9vz3OC0xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.describe()"
      ],
      "metadata": {
        "id": "8Qm8RmNjC2mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualizing a dataset"
      ],
      "metadata": {
        "id": "H25RRmogC5ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "blzRS3mxC30F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.plot(x='housing_median_age', y='median_house_value', style='o', alpha=0.2)\n",
        "plt.title('RM vs MEDV', fontsize=16)\n",
        "plt.xlabel('RM')\n",
        "plt.ylabel('MEDV')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kfVe2DSuC68V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Curve Fitting\n",
        "A fundamental concept in Pattern recognition is curve fitting. This allows our programs to do approximations and optimizations given a data set. For this section, we will use [SciKit Learn](https://scikit-learn.org/stable/index.html). SciKit Learn is one of the most valuable libraries for Data Science and Machine Learning Engineering in modeling learning algorithms. It has a range of APIs that will significantly assist in Data wrangling, data validation, supervised learning, and unsupervised learning. Check out these SciKit learn [tutorials](https://scikit-learn.org/stable/user_guide.html) for better understanding."
      ],
      "metadata": {
        "id": "oNuU4M8gDFWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scikit-learn\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "UG4GY6-ZC-DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Linear Regression\n",
        "Linear regression is one of the fundamental and easiest curve-fitting techniques in Pattern Recognition. Long story short, linear regression finds the best-fit first-order polynomial to a given dataset $X$. This line is represented as:\n",
        "$$y = \\omega X+b$$\n",
        "A linear regression algorithm or a linear regressor $y$ learns the weights $\\omega$ and bias $b$."
      ],
      "metadata": {
        "id": "VysUmo3QDLhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = ds['population'].values.reshape(-1,1)\n",
        "y = ds['total_bedrooms'].values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "wLFQ6ZDvDWst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)"
      ],
      "metadata": {
        "id": "-i6bdAICDKZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "SNWJv8oIDS4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_summary = pd.DataFrame(['population'], columns=['Features'])\n",
        "model_summary['Weights Raw'] = model.coef_\n",
        "model_summary = pd.concat([model_summary, pd.DataFrame({'Features':['Intercept'], 'Weights Raw':[float(model.intercept_)]})], ignore_index=True)\n",
        "model_summary"
      ],
      "metadata": {
        "id": "W2oL_-5BDXpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here we can interpret this as the regressor as:\n",
        "$$y_{\\text{total bedrooms}} = \\omega_{\\text{population}}X+b \\\\\n",
        "y_{\\text{total bedrooms}} = 0.321\\cdot X + 80.366$$"
      ],
      "metadata": {
        "id": "ZCNEow7cEVKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_test)\n",
        "out = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': preds.flatten()})\n",
        "out"
      ],
      "metadata": {
        "id": "nEbUm0D_DZjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Predictions', fontsize=16)\n",
        "\n",
        "plt.scatter(preds, y_test, s = 50,  alpha=0.4)\n",
        "plt.xlabel('Ground Truth', fontsize=10)\n",
        "plt.ylabel('Prediction', fontsize=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C2kdUfZ0EYcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X_test, y_test,  s = 50, alpha=0.5)\n",
        "plt.plot(X_test, preds, color='red', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l1rmn2j9EaJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Formula: Adjusted $R^2$</b><br>\n",
        "$R^2_{adj.} = 1-(1-R^2)*\\frac{n-1}{n-p-1}$\n",
        "\n",
        "Whereas: p = Predictors; n = Observations"
      ],
      "metadata": {
        "id": "3gkQkOvtEglp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjr2(r2,x):\n",
        "    n = x.shape[0]\n",
        "    p = x.shape[1]\n",
        "    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
        "    return adjusted_r2"
      ],
      "metadata": {
        "id": "SE7LEJ7BEd2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = metrics.mean_squared_error(y_test, preds)\n",
        "RMSE = np.sqrt(MSE)\n",
        "R2 = metrics.r2_score(y_test, preds)\n",
        "AR2 = adjr2(R2,X_train)\n",
        "model_metrics = pd.DataFrame([['MSE'],['RMSE'],['R^2'],\n",
        "                              ['Adjusted R^2']],\n",
        "                             columns=['Metrics'])\n",
        "model_metrics['Simple Regression'] = MSE, RMSE, R2, AR2\n",
        "model_metrics"
      ],
      "metadata": {
        "id": "P1mafZ4jEkN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Multiple Linear Regression\n",
        "Taking linear regressors to the new level, weâ€™ll use Multiple Linear Regression. In a multiple linear regressor, we take $n$ number of parameters or features to describe a target $y$. We can represent this as:\n",
        "$$y = \\omega_1X + \\omega_2X + ... +\\omega_nX + b$$"
      ],
      "metadata": {
        "id": "o7RfjYN6EnNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.plot(x='population', y='total_bedrooms', style='o', alpha=0.2)\n",
        "plt.title('Population vs Total Bedrooms', fontsize=16)\n",
        "plt.xlabel('Population')\n",
        "plt.ylabel('Total Bedrooms')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9dTFy4P3ElLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.plot(x='households', y='total_bedrooms', style='o', alpha=0.2)\n",
        "plt.title('Number of households vs Total bedrooms', fontsize=16)\n",
        "plt.xlabel('Number of Households')\n",
        "plt.ylabel('Total bedrooms')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhlX1OWHEpUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(np.c_[ds['population'], ds['households']], columns=['population','households'])\n",
        "y = ds['total_bedrooms']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "a2nB7Zn3Eqgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_summary = pd.DataFrame(X.columns, columns=['Features'])\n",
        "model_summary['Weights Raw'] = model.coef_.reshape(2,1)\n",
        "model_summary = pd.concat([model_summary, pd.DataFrame({'Features':['Intercept'], 'Weights Raw':[float(model.intercept_)]})], ignore_index=True)\n",
        "model_summary"
      ],
      "metadata": {
        "id": "FYlISOIJEtkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_test)\n",
        "out = pd.DataFrame({'Actual': y_test, 'Predicted': preds})\n",
        "out"
      ],
      "metadata": {
        "id": "1Nk-kpfQFD8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Predictions', fontsize=16)\n",
        "\n",
        "plt.scatter(y_test, preds, s = 50,  alpha=0.4)\n",
        "plt.xlabel('Ground Truth')\n",
        "plt.ylabel('Prediction', fontsize=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u_OEIxJqFMNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = metrics.mean_squared_error(y_test, preds)\n",
        "RMSE = np.sqrt(MSE)\n",
        "R2 = metrics.r2_score(y_test, preds)\n",
        "AR2 = adjr2(R2,X_train)\n",
        "model_metrics['Multiple Regression'] = MSE, RMSE, R2, AR2\n",
        "model_metrics"
      ],
      "metadata": {
        "id": "JsESJZFqFPQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "site1 = np.array([[5126, 1270]])\n",
        "model.predict(site1)"
      ],
      "metadata": {
        "id": "y9644_ZgFQtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Gradient Descent\n",
        "Diving deeper into the Machine Learning rabbit hole, we need to discuss the fundamental technique in machine learningâ€”Gradient Descent. Gradient descent is an optimization algorithm used to minimize functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, gradient descent is used to update the parameters of models. In this section, we'll try to apply this algorithm with the fundamental unit of a neural networkâ€”the Perceptron."
      ],
      "metadata": {
        "id": "wyk_z6_AFUpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Perceptron Algorithm\n",
        "The Perceptron was first conceptualized by Frank Rosenblatt in his paper [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) in 1958. The perceptron is one of the earliest mathematical conceptualization of a brain neuron. In simplest terms, a perceptron does a weighted sum of all inputs and then performs an activation. In the early implementations of the perceptron the activation used was a step function described as:\n",
        "$$step(z) = \\left\\{\n",
        "  \\begin{array}\\\\\n",
        "    1 \\text{ if } \\ b+ \\sum w_iX_n\\geq 0 \\\\\n",
        "    0 \\text{ otherwise}\n",
        "    \\end{array}\n",
        "\\right.\n",
        "$$\n",
        "![image](https://jontysinai.github.io/assets/article_images/2017-11-11-the-perceptron/bio-vs-MCP.png)\n"
      ],
      "metadata": {
        "id": "QpuJQr3pFahg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's create a dummy dataset for binary classification."
      ],
      "metadata": {
        "id": "NpiPHnIGFz9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 100\n",
        "m1 = np.array([-2, 0]).T\n",
        "m2 = np.array([2, 0]).T\n",
        "S = np.identity(2)"
      ],
      "metadata": {
        "id": "bVGTo2-DFSRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "X_train = np.array([np.random.multivariate_normal(m1,S,int(N/2)),np.random.multivariate_normal(m2,S,int(N-(N/2)))]).T\n",
        "y_train = np.array([np.ones(int(N/2)), np.zeros(int(N-(N/2)))]).reshape((-1,1))\n",
        "X_train = np.concatenate((X_train[0], X_train[1]), axis=0)"
      ],
      "metadata": {
        "id": "0Q8pRd-pF1VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0:5]"
      ],
      "metadata": {
        "id": "vTTKGe4jF2o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "rQPdMp6tF32H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0:5]"
      ],
      "metadata": {
        "id": "Wic-J3zCF5bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "lNUCU7nFF6zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(100)\n",
        "M = N*0.4\n",
        "X_test = np.array([np.random.multivariate_normal(m1,S,int(M/2)),np.random.multivariate_normal(m2,S,int(M-(M/2)))]).T\n",
        "X_test = np.concatenate((X_test[0], X_test[1]), axis=0)\n",
        "y_test = np.array([np.ones(int(M/2)), np.zeros(int(M-(M/2)))]).reshape((-1,1))"
      ],
      "metadata": {
        "id": "dFs5QG-0F77Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(X):\n",
        "  plt.figure(figsize=(8,8))\n",
        "  mid = int(X.shape[0]/2)\n",
        "  plt.scatter(X[:mid,0], X[:mid,1], c='r', label='1')\n",
        "  plt.scatter(X[mid:,0], X[mid:,1], c='b', label='0')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "uG1bQd87F8_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(X_train)"
      ],
      "metadata": {
        "id": "SB1M_bvKGAMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(X_test)"
      ],
      "metadata": {
        "id": "Jn5d6E_FGBtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step_activation(z):\n",
        "  \"\"\"\n",
        "  Compute the step activation of z\n",
        "\n",
        "  Arguments:\n",
        "  z -- A scalar or numpy array of any size.\n",
        "\n",
        "  Return:\n",
        "  filtered step activations step(z)\n",
        "  \"\"\"\n",
        "  return np.where(z>=0, 1,0)"
      ],
      "metadata": {
        "id": "IKhPG6aMGDVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(-0.5,0.5,0.01)\n",
        "plt.plot(x, step_activation(x))\n",
        "plt.ylabel('Activation')\n",
        "plt.xlabel('Input Feature')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xylLkSVcGEy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(dim):\n",
        "  \"\"\"\n",
        "  Does a zero-initialization of the weights and bias\n",
        "\n",
        "  Arguments:\n",
        "  dim -- Desired dimension for the weights.\n",
        "\n",
        "  Return:\n",
        "  w -- initialized weights\n",
        "  b -- initilaized bias\n",
        "  \"\"\"\n",
        "  w = np.zeros(shape=(dim,1))\n",
        "  b = 0\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "LUpdRE6bGdAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_err(preds,y):\n",
        "  \"\"\"\n",
        "  Computes the Sum of Squared Errors for a set of predictions\n",
        "  and truth values\n",
        "\n",
        "  Arguments:\n",
        "  preds -- Set of predictions.\n",
        "  y -- Set of truth values\n",
        "\n",
        "  Return:\n",
        "  sse -- Sum of the squared errors\n",
        "  \"\"\"\n",
        "  sse = np.sum(np.square(y-preds))\n",
        "  return sse\n"
      ],
      "metadata": {
        "id": "fKHZGSR9GuhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, Y):\n",
        "  \"\"\"\n",
        "  Computes the accuracy for a set of predictions\n",
        "  and truth values\n",
        "\n",
        "  Arguments:\n",
        "  preds -- Set of predictions.\n",
        "  y -- Set of truth values\n",
        "\n",
        "  Return:\n",
        "  accuracy -- Computed accuracy\n",
        "  \"\"\"\n",
        "  accuracy = 1-np.mean(np.abs(preds-Y))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "OxR5WzcvGwLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(X,y,w,b):\n",
        "\n",
        "  # Compute for the transformed vector of the\n",
        "  # dataset w.r.t the weights and biases\n",
        "  z = (X@w) + b\n",
        "\n",
        "  # Compute for the step activation\n",
        "  A = step_activation(z)\n",
        "\n",
        "  # Compute for the prediction error\n",
        "  error = A-y\n",
        "  acc = accuracy(y,A)\n",
        "\n",
        "  # Update the weights and biases\n",
        "  # Learning/Update routine\n",
        "  w = np.dot(X.T,error)\n",
        "  b = np.sum(error)\n",
        "\n",
        "  # Compute the cost\n",
        "  cost = sum_err(A,y)\n",
        "\n",
        "  # Store the parameters in a dictionary for tracking\n",
        "  grads = {\"dw\": w,\n",
        "           \"db\": b}\n",
        "\n",
        "  return grads, cost, acc"
      ],
      "metadata": {
        "id": "Y6KjpoTNGxfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b = init_weights(X_train.shape[1])\n",
        "propagate(X_train,y_train,w,b)"
      ],
      "metadata": {
        "id": "ha_Q23LjGyx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "def train(w, b, X, y, lr, epochs, early_stopping=True, stop_thresh=0.9):\n",
        "  costs = []\n",
        "  accuracies = []\n",
        "\n",
        "  for i in tqdm(range(epochs)):\n",
        "    # Do a forward propagation to obtain the gradients\n",
        "    grads, cost, accuracy = propagate(X,y,w,b)\n",
        "\n",
        "    # Locally store the gradients\n",
        "    dw=grads['dw']\n",
        "    db=grads['db']\n",
        "\n",
        "    # Update routine per epoch\n",
        "    w = w - lr*dw\n",
        "    b = b - lr*db\n",
        "\n",
        "    # Store the costs per epoch for logs\n",
        "\n",
        "    # print (f\"Epoch {i}: Loss: {cost} Accuracy: {accuracy}\")\n",
        "    costs.append(cost)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Store the learned parameters for logs\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    if early_stopping and accuracy >= stop_thresh:\n",
        "      print(f\"Target metric met, stopping the training at {i} epoch(s).\\n\")\n",
        "      break\n",
        "\n",
        "  return params, grads, costs\n"
      ],
      "metadata": {
        "id": "yNU7wS8iG0Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b = init_weights(X_train.shape[1])\n",
        "learning_rate = 1\n",
        "epochs = 100\n",
        "\n",
        "params, grads, ff_costs = train(w, b, X_train, y_train,\n",
        "                             lr=learning_rate, epochs=epochs,\n",
        "                             early_stopping=True, stop_thresh=1.0)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "metadata": {
        "id": "7B5k2hXcG1o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(epochs), ff_costs, 'bo-')\n",
        "plt.ylabel('Training Cost')\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qYxZa-p3G3iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, weights, bias):\n",
        "  z = (X@weights)+bias\n",
        "  return np.where(z>=0, 1,0)"
      ],
      "metadata": {
        "id": "g3PgJyFSH1tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = params[\"w\"]\n",
        "bias = params[\"b\"]\n",
        "preds = predict(X_test,weights,bias)\n",
        "accuracy(y_test, preds)"
      ],
      "metadata": {
        "id": "fkMxeVBoH_Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "c_matrix = confusion_matrix(y_test, preds)\n",
        "sns.heatmap(c_matrix, annot=True)\n",
        "plt.xlabel(\"Ground Truths\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kVNY5W6EIDKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "print(f\"F1 Score: \\t{f1_score(y_test, preds)}\")\n",
        "print(f\"Recall: \\t{recall_score(y_test, preds)}\")\n",
        "print(f\"Precision: \\t{precision_score(y_test, preds)}\")"
      ],
      "metadata": {
        "id": "_4Ma36z6Ijom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_weights(X,w,b):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.scatter(X[:int(X.shape[0]/2),0], X[:int(X.shape[0]/2),1],\n",
        "              s = 50, color='blue', alpha=0.5, label=1)\n",
        "  plt.scatter(X[int(X.shape[0]/2):,0], X[int(X.shape[0]/2):,1],\n",
        "              s = 50, color='red', alpha=0.5, label=0)\n",
        "  x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
        "  linex = np.linspace(x_min, x_max)\n",
        "  liney = -w[0]/w[1] * linex - b/w[1]\n",
        "  plt.plot(linex, liney, label='decision bounday')\n",
        "  plt.legend()\n",
        "  plt.axhline(color='black')\n",
        "  plt.axvline(color='black')\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JCv3Af4ZIq52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_weights(X_train,params['w'],params['b'])"
      ],
      "metadata": {
        "id": "kntKwF5RIsbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Gradient with Backpropagation\n",
        "Although the Perceptron with the step activation produces good results producing a linear classifier, it lacks another fundamental technique for being a robust neural network modelâ€”Backpropagation. Backpropagation is a short form for \"backward propagation of errors.\" It is a method of training artificial neural networks. This method helps to calculate the gradient of a loss function for all the weights in the network. <br>\n",
        "In this section, we will use a sigmoid function as an activation function instead of a step activation. Since backpropagation will not be effective with the step function its gradient (derivative) is zero, and that will not be useful for computing the loss function.<br>\n",
        "<b>Loss Function</b><br>\n",
        "A loss function is the function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function [[1]](https://www.deeplearningbook.org/contents/numerical.html). <br>\n",
        "To save you the time and brainpower, for our example our loss function is:\n",
        "$$J(\\theta)=\\frac{1}{m} \\sum^m_{i}cost(h_{\\theta}(x^{(i)}, y^{(i)}) \\\\\n",
        "\\text{if y = 1} : -\\log{(h_\\theta(x))}\\\\\n",
        "\\text{if y = 0} : -\\log{(1-h_\\theta(x))}\\\\\n",
        "J(\\theta)=-\\frac{1}{m} \\sum^m_{i}{y^{(i)}\\log{(h_\\theta(x))}+(1-y^{(i)})(\\log{(1-h_\\theta(x))}} \\\\\n",
        "J(\\theta)=-\\frac{1}{m} \\sum^m_{i}{Y^T\\log(h)+(1-Y)^T\\log(1-h)}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "jMtqKqGzIv-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    return s"
      ],
      "metadata": {
        "id": "-U3t-vyhIuKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(-10,10,0.01)\n",
        "plt.plot(x, sigmoid(x))\n",
        "plt.ylabel('Activation')\n",
        "plt.xlabel('Input Feature')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cm-pxEOgIzKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_derivative(d):\n",
        "  return d*(1.0-d)"
      ],
      "metadata": {
        "id": "EKvurE8GI9hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size\n",
        "    Y -- true \"label\" vector\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    alpha = 10**-8\n",
        "\n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    h = sigmoid((X@w)+b)                                   # compute activation\n",
        "    J = -1 / m * np.sum(Y * np.log(h+alpha) + (1-Y) * np.log((1-h)+alpha))  # compute cost\n",
        "    error = (h-Y)*transfer_derivative(h)\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = 1/m * X.T @ error\n",
        "    db = 1/m * np.sum(error)\n",
        "\n",
        "    cost = np.squeeze(J)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return grads, cost"
      ],
      "metadata": {
        "id": "5w0l_L8tI-KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, epochs, lr, print_cost = True):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape\n",
        "    Y -- true \"label\" vector\n",
        "    epochs -- number of iterations of the optimization loop\n",
        "    lr -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in tqdm(range(epochs)):\n",
        "\n",
        "\n",
        "        # Cost and gradient calculation\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        w = w - lr * dw\n",
        "        b = b - lr * db\n",
        "\n",
        "        # Record the costs\n",
        "        costs.append(cost)\n",
        "\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "8a9IjLcMJAGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b = init_weights(X_train.shape[1])\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "params, grads, bp_costs = optimize(w, b, X_train, y_train,\n",
        "                             lr=learning_rate, epochs=epochs)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "metadata": {
        "id": "CXhRK_aPJCMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(epochs), ff_costs, 'b-')\n",
        "plt.plot(np.arange(epochs), bp_costs, 'r-')\n",
        "\n",
        "plt.ylabel('Training Cost')\n",
        "plt.xlabel('Epoch')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BjvK6TjMJhx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "\n",
        "    A = sigmoid((X@w)+b)\n",
        "    Y_prediction = np.where(A>=0.5,1,0)\n",
        "\n",
        "    return Y_prediction"
      ],
      "metadata": {
        "id": "oR3L8dvfKCKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 10, learning_rate = 0.5, print_cost = True):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize parameters with zeros\n",
        "    w, b = init_weights(X_train.shape[1])\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "v4cjNRX4KDS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron_model = model(X_train, y_train, X_test, y_test, num_iterations=100, learning_rate=1)"
      ],
      "metadata": {
        "id": "8ieYrt-3KFNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_matrix = confusion_matrix(y_test, neuron_model['Y_prediction_test'])\n",
        "sns.heatmap(c_matrix, annot=True)\n",
        "plt.xlabel(\"Ground Truths\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ouun3wh1KIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"F1 Score: \\t{f1_score(y_test, neuron_model['Y_prediction_test'])}\")\n",
        "print(f\"Recall: \\t{recall_score(y_test, neuron_model['Y_prediction_test'])}\")\n",
        "print(f\"Precision: \\t{precision_score(y_test, neuron_model['Y_prediction_test'])}\")"
      ],
      "metadata": {
        "id": "cHvkKR2-KNTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_weights(X_train,neuron_model['w'],neuron_model['b'])"
      ],
      "metadata": {
        "id": "57BKM_LwKNt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Up Next: Artificial Neural Networks\n",
        "![image](https://www.researchgate.net/profile/Sandra_Vieira5/publication/312205163/figure/fig1/AS:453658144972800@1485171938968/a-The-building-block-of-deep-neural-networks-artificial-neuron-or-node-Each-input-x.png)\n",
        "<br><i>Image from: Vieira, Sandra & Pinaya, Walter & Mechelli, Andrea. (2017). [Using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: Methods and applications.](https://www.researchgate.net/publication/312205163_Using_deep_learning_to_investigate_the_neuroimaging_correlates_of_psychiatric_and_neurological_disorders_Methods_and_applications) Neuroscience & Biobehavioral Reviews. 74. 10.1016/j.neubiorev.2017.01.002."
      ],
      "metadata": {
        "id": "sFRY9RAPKTxi"
      }
    }
  ]
}